IMDB Movie Rating Final Project

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r}
library(tidyverse)
library(broom)
library(ggplot2)
```

```{r}
data <- read.csv("../data/movie_metadata.csv")
view(data)
```

The first part of data cleaning involves removal of spurious characters (Â) from a the movie title, genre and plot keyword columns.

Then we remove the duplicates in the data, using then "movie_title" column. Duplicate data will skew our analysis hence needs to be removed.

Third part of data cleaning is the currency columns. The "Budget" and "Gross" (revenue) columns for a few countries were not converted to USD while compiling the data. Hence we have done this manually. This cleaning will help us compare these financial columns across movies and countries. All the currency rates were taken from (http://www.xe.com/) as of November 2022.

Lastly we created a profit_flag column, which is 1 if the movie is profitable (Revenue > Budget) and 0 otherwise.


```{r}
data$movie_title <- (sapply(data$movie_title,gsub,pattern="\\Â",replacement=""))
data$genres_2 <- (sapply(data$genres,gsub,pattern="\\|",replacement=" "))
data$plot_keywords_2 <- (sapply(data$plot_keywords,gsub,pattern="\\|",replacement=" "))

data = data[!duplicated(data$movie_title),]

data <- transform(data, budget = ifelse(country == "South Korea", budget/1335.19, budget))
data <- transform(data, budget = ifelse(country == "Japan", budget/139.11, budget))
data <- transform(data, budget = ifelse(country == "Turkey", budget/18.61, budget))
data <- transform(data, budget = ifelse(country == "Hungary", budget/392.69, budget))
data <- transform(data, budget = ifelse(country == "Thailand", budget/35.69, budget))


data <- transform(data, gross = ifelse(country == "South Korea", gross/1335.19, gross))
data <- transform(data, gross = ifelse(country == "Japan", gross/139.11, gross))
data <- transform(data, gross = ifelse(country == "Turkey", gross/18.61, gross))
data <- transform(data, gross = ifelse(country == "Hungary", gross/392.69, gross))
data <- transform(data, gross = ifelse(country == "Thailand", gross/35.69, gross))


data$profit_flag <- as.factor(ifelse((data$gross > data$budget),1,0))

view(data)
```


Each movie in our dataset had more than one genre, hence some cleaning and genre separation was required. Some pre-built functions in the TM package were very useful. In short, in the Analysis of Genre, the following was done:
  1. Cleaning of the Genre Variable
  2. Converting Genre variable to Corpus
  3. Frequency Analysis: See which are the most used genres in a movie

Drama, Comedy and Thriller are the top movie genres, as shown in the below Word-Cloud and Bar-Chart

```{r}
library(tm)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(plyr)
genre <- Corpus(VectorSource(data$genres_2))
genre_dtm <- DocumentTermMatrix(genre)
genre_freq <- colSums(as.matrix(genre_dtm))
freq <- sort(colSums(as.matrix(genre_dtm)), decreasing=TRUE) 
genre_wf <- data.frame(word=names(genre_freq), freq=genre_freq)


ggplot(genre_wf, aes(x=reorder(word,-freq), y=freq))+ 
  geom_bar(stat="identity")+
  theme(axis.text.x=element_text(angle=45, hjust=1))+
  ggtitle("Movie Genre frequency graph")+
  xlab("Genre")+
  ylab("Frequency") + 
  labs(alt = "Bar graph of Movie Genre frequency where drama is the most common")
```
```{r, fig.alt = "Word Cloud of movie genre frequency where Drama is the biggest"}
set.seed(10)
pal2 <- brewer.pal(8,"Dark2")
wordcloud(genre_wf$word,genre_wf$freq,random.order=FALSE,
          rot.per=.15, colors=pal2,scale=c(4,.9),
          title="WordCloud: Movie Genres") 
```

# Exploratory Data Analysis

We have explored the budget variable in the proposal and the genre variable in the previous parts which are some of the explanatory variables that we think that they should be included in the model for predicting the movie rating. In this part we will perform more Exploratory Data Analysis (EDA) to help us understand the data better moving to the modeling part. 

## Summary statistics

The first step of EDA is to create the summary statistics of the dataset by calling the summary function. The following the summary statistics table provides the information about the minimum, first quantile, median, mean, third quantile, maximum, and the number of NaN value of each variable. Mean and median are the measure of central tendency. 

Comparing them gives us the idea of the distribution without using the visualization. If the mean and median is close to each other, the distribution is normal. For example the difference between the mean and median is small, then the duration data is normally distributed. If the mean is higher than the median, the distribution is right-skewed. In this case, the actor_1_facebook_likes is right-skewed because the mean is much higher than the median.

```{r}
summary(data)
```
## Correlation

The second statistic that we will explore is the correlation. However, the correlation matrix can only be used with the numerical variables. Therefore, we create another dataset call "mydata" which only contains the numerical variables. The c() function will create vector that according the specified columns, which are the numerical columns.

```{r}
mydata <- data[, c(3,4,5,6,8,9,13,14,16,19,23,24,25,26,27)]
head(mydata)
```

After that we drop the NaN value in the mydata dataset and find the correlation coefficient by using the cor() function. We also around the number to 2 decimals to make them easier to look at in the correlation matrix.
```{r}
mydata <- na.omit(mydata) # DROP NaN value
cormat <- round(cor(mydata),2)
head(cormat)
```
Before we create the correlation matrix, we have to reshape the data. We use melt() function to stack the columns together as the table below.
```{r}
library(reshape2)
re_cormat <- melt(cormat)
head(re_cormat)
```
Finally, we create correlation matrix by using the geom_tile() function from ggplot library. 

The correlation matrix gives us the correlation coefficient of each pair of variables that tells us whether the two variables is linearly related. The coefficient is between +1 and -1. The negative(-) refers to the inverse variation and the higher magnitude, the stronger linear relationship between variables. We can use this correlation to select the potential numerical variables to help create the model for the rating recommendation system by choosing the variables that has a high correlation coefficient. 
```{r}
ggplot(data = re_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile(color = "white") + 
  geom_text(aes(label = value), color = "white", size = 2) + # put the text values of each correlation
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + # rotate the x-label for easier reading
  coord_fixed() + # createline across each correlation
  labs(
    title = "Correlation Matrix of numerical variables",
    alt = "Heat Tile map of correlation in numerical values to show relationship between variables"
  )
```

## Visualizations
The third part of the EDA is the visualization. We will use the visualizations to help us understand other information about the dataset that is not shown in the summary statistic and the correlation matrix.
 
Since the model will predicting the rating which is the IMDB Score, we will explore the relationship between the IMDB Score with Content Rating variables. Therefore, the first visualization is the boxplot of the IMDB Score by Content Rating. We can see that each boxplot represents each type of the content. The boxplots gives the spread of the data and the outliers. Each content has different spread and distribution, but the most median rating of each content type are in the rating range between 6.25 to 7.5. This visualization could infer that the content type(rating) might not have big influence on the IMDB Score. 
```{r}
ggplot(data, aes(x = content_rating, y = imdb_score)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(title = "IMDB Score by Content Rating",
       alt = "Box plots for each content rating type compared to IMDB score to show distribution")

```

Next we would like to explore the relationship between the rating and the director Therefore, the second visualization is the Scatter plot between the IMDB Score and the median of the Director Facebook Likes. We can see that the movie that tends to have high rating also has the high facebook likes for the director. Therefore, the number of facebook likes from the director could affect the movie rating.
```{r}
ggplot(data, aes(x = imdb_score, y = director_facebook_likes)) +
  geom_point(stat = 'summary', fun = 'mean')  +
  labs(
    title = "IMDB Score VS median of the Director Facebook Likes",
    alt = "Scatterplot between the IMDB Score and the median of the Director Facebook Likes as it increases exponentially "
  )

```
Next, we wanted to see the relationship between title year and the gross revenue of a movie. We can see from the scatter plot that there has been an increase in the number of movies released as well as the gross that is being generated. The profit flag variable also indicates the amount of profitable films in that time frame.
```{r}
 ggplot(data, aes(x = title_year, y = (gross / 100000), color = profit_flag)) +
   geom_point(alpha = .6) + 
   labs(
     y = "Gross($100,000)",
     title = "Scatter plot between the gross and the title_year",
     alt = "Scatter plot between the gross and the title_year showing if a movie is profitable"
   )
```

What is the relationship between movie duration and IMDB score? Do these factors have an effect on the profit? We built a scatter plot to see how duration impacts IMDB score and if these variables effect the profit of a movie.
We can see that most films have a duration between 50 to 150 mins. Movies with high IMDB scores tend to be more profitable. The line shows the trend in the plot.
```{r}
ggplot(data, aes(x = duration, y = imdb_score))+ 
  geom_point(aes(color = profit_flag))+ 
  geom_smooth(method="loess", se=F) + 
  labs(title="Duration Vs IMDB Score", 
       x="Duration", 
       y="IMDB Score",
       alt = "Scatterplot showing relationship between movie duration and IMDB score with trend line to see how the score changes")

```
Next, we wanted to see how gross revenue is distributed in this data set. From the density plot below, we can see that the revenue average for not profitable movies is less than that of profitable movies which is to be expected. Also, the profitable movies are more evenly spread in gross compared to not profitable movies.

```{r}

ggplot(data, aes(x = gross, fill = profit_flag)) +
  geom_density(
    adjust = 2,
    alpha = 0.5) +
  xlim(0, 300000000)+
  labs(
    x = "Gross Revenue",
    y = "Density",
    title = "Gross Revenue by Profitability",
    alt = "Density plot of gross revenue and if it is profitable showing the wide spread of profitable movies")
```
Data Model:

The next thing that we wanted to do in our analysis was to create a model that could predict IMDB score based on the different variables in our data set. Based on the Corrgram below we created a model.
```{r}
require(corrgram)
corrgram(data, order=TRUE)

```
We decided to split the training and test data set into a 70-30 split. We used linear regression on the training data set to create our model object. 

```{r}
dt = sort(sample(nrow(data), nrow(data)*.7))
train<-data[dt,]
test<-data[-dt,]
dim(train)

```
```{r}
lmModel <- lm(imdb_score ~ duration+gross+title_year, data = train)

# Printing the model object
print(lmModel)
```

```{r}
summary(lmModel)
```
This combination of variables wasn't creating a very accurate model so we tried including more categorical variables.

The new model included categorical variables and we checked for variables with p-values lower than 0.05.

```{r}
new_lmModel <- lm(imdb_score ~ gross + budget  + title_year  +num_voted_users+ num_critic_for_reviews + duration + profit_flag, data = train)
# Printing the model object
print(new_lmModel)
```
```{r}
summary(new_lmModel)
```
This model has a better R-squared value which makes this more accurate.
The Residuals plot below shows us the difference between the observed and the fitted model points. As we can see a pattern in the residual plot, this indicates that a linear model might not be ideal in this situation. 
```{r}
ggplot(new_lmModel, mapping = aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = "dashed") +
  labs(x = "IMDB Score", y = "Residuals")
```

When we compare the predicted values, they are not very accurate but are pretty close.
```{r}
head(test$imdb_score)
head(predict(new_lmModel, test))
```
Overall, we found that this data set doesn't contain all the information required for us to create the most accurate prediction of IMDB score. We need a more vast data set and other variables to be able to predict IMDB Score. 
